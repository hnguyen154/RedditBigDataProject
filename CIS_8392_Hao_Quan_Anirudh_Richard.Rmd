---
title: "REDDIT SENTIMENT PROJECT"
author: "Richard More | Quan Le | Hao Nguyen | Anirudh Chaudhary"
date: "Date: 02/19/2020"
output: 
  html_document:
    theme: readable
    highlight: zenburn
    toc: true
    toc_depth: 3
    toc_float: 
      collapsed: false
header-includes:
  - \hypersetup{colorlinks=true, linkcolor=blue}
  - \usepackage{sectsty}
---

\allsectionsfont{\centering}
\subsectionfont{\raggedright}
\subsubsectionfont{\raggedleft}

<style type="text/css">

h1.title {
  font-size: 35px;
  color: DarkRed;
  text-align: center;
}
h4.author { 
    font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date { 
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}

</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, warning=FALSE, message=FALSE}
library(RedditExtractoR)
library(tidyverse)
library(wordcloud)
library(tidytext)
library(recipes)
library(skimr)
library(h2o)
library(readr)
library(RCurl)
```

## Reddit Final Project

### 1. Reddit Sentiment Analysis

#### Retrieve Reddit Information

```{r gather_data, warning=FALSE, message=FALSE, results="hide"}
r <- reddit_urls(subreddit = "Coronavirus", page_threshold = 10)
rc <- reddit_content(r$URL)
rc_rel <- rc[c("comment", "comment_score", "post_score")]
rc_rel <- mutate(rc_rel, id = rownames(rc_rel))
```

#### Clean and transform sentiment

```{r claning}
tokens <- rc_rel %>%
  unnest_tokens(output = word, input = comment)

sw = get_stopwords()
cleaned_tokens <- tokens %>%
  filter(!word %in% sw$word)

nums <- cleaned_tokens %>%
  filter(str_detect(word, "^[0-9]|http[s]?|.com$")) %>%
  select(word) %>% unique()

cleaned_tokens <- cleaned_tokens %>%
  filter(!word %in% nums$word)

rare <- cleaned_tokens %>%
  count(word) %>%
  filter(n < 10) %>%
  select(word) %>%
  unique()

cleaned_tokens <- cleaned_tokens %>%
  filter(!word %in% rare$word)
```

#### B. WordCloud Visualization of World News

Wordcloud all cleaned tokens

```{r wordcloud}
pal <- brewer.pal(8,"Dark2")
cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = pal))
```

Join tokens with Bing, NRC, and Afinn

```{r sentiments, warning=FALSE, message=FALSE}
sent_reviews = cleaned_tokens %>%
  left_join(get_sentiments("nrc")) %>%
  rename(nrc = sentiment) %>%
  left_join(get_sentiments("bing")) %>%
  rename(bing = sentiment) %>%
  left_join(get_sentiments("afinn")) %>%
  rename(afinn = value)
```

#### C. WordCloud Visualization of Corona Virus

Sentiment for the word "corona" is not present in the above mentioned 3 sentiment lexicons, so we have to add our own.

```{r corona_sentiment}
sent_reviews <- sent_reviews %>%
  mutate(bing = ifelse(word == "corona", "negative", bing)) %>%
  mutate(nrc = ifelse(word == "corona", "negative", nrc))
```

Wordcloud with Bing Lexicon

```{r sentiment_bing}
bing_word_counts <- sent_reviews %>%
  filter(!is.na(bing)) %>%
  count(word, bing, sort = TRUE)

bing_word_counts %>%
  filter(n > 250) %>%
  mutate(n = ifelse(bing == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = bing)) +  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

Wordcloud with NRC Lexicon

```{r sentiment_nrc_wordcloud}
nrc_word_counts <- sent_reviews %>%
  filter(!is.na(nrc)) %>%
  count(word, nrc, sort = TRUE)

nrc_word_counts %>%
  filter(n > 250) %>%
  mutate(n = ifelse(nrc %in% c("negative", "fear", "anger", "sadness", "disgust"), -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = nrc)) +  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

Distribution of sentiments

```{r sentiment_nrc_graph}
sent_reviews %>%
  filter(!is.na(nrc)) %>%
  group_by(nrc) %>%
  summarise(count = n()) %>%
  ggplot(., aes(x = reorder(nrc, -count), y = count)) +
    geom_bar(stat = "identity") +
    xlab("NRC") +
    labs(title = "Distribution of sentiments")
```

#### D. Chart for Concern of Corona Virus Overtime on Reddit

Set up the code to aggregate number of comment about World News and Corona everyday
```{r}
# Aggregate Corona Virus
df_corona <- rc %>% filter(str_detect(title,"corona") | 
                             str_detect(title,"Corona") |
                             str_detect(title,"CORONA") |
                             str_detect(title,"2019-nCOV") |
                             str_detect(title,"COVID-19") |
                             str_detect(title,"Covid-19") |
                             str_detect(title,"covid-19")) %>%
  select(comm_date) %>%
  group_by(comm_date) %>% 
  count(comm_date)

#change the file Name of Corona
c_name <- c("Date","Count_Corona")
colnames(df_corona) <- c_name

# Aggregate World
df_world <- rc %>% 
  select(comm_date) %>%
  group_by(comm_date) %>%
  count(comm_date)

#change the file Name of World
w_name <- c("Date","Count_World")
colnames(df_world) <- w_name

df_export <- inner_join(df_world,df_corona, by ="Date")

if (file.exists("corona_over_time.csv") == TRUE){
  write.table(df_export,"corona_over_time.csv",
              append = TRUE, col.names = FALSE, sep =",", row.names=FALSE)
} else {
  write.table(df_export,"corona_over_time.csv",
              append = FALSE, col.names = TRUE, sep =",", row.names=FALSE)
}
```

Code to create a schedule for Corona aggregation
```{r}
#Set schedule everyday to feed data

#install.packages("taskscheduleR")
#library(taskscheduleR)
#myscript <- system.file("reddit_corona.R")

#taskscheduler_create(taskname = "CORONA_UPDATE", 
                     #rscript = "C:/Users/JNK4QBK/Desktop/reddit_corona.R",schedule = "DAILY", 
                     #starttime = "23:50", startdate = format(Sys.Date()+1, "%d/%m/%Y"))
```

Import csv of Corona aggregation and visualize by barchart + line
```{r}
# urlfile="https://github.com/hnguyen154/RedditBigDataProject/blob/master/corona_over_time.csv"
# 
# #Get the csv from Github
# x <- getURL(urlfile)
# df_corona_plot <- read.csv(text = x)
# 
# #df_corona_plot <- read.csv("corona_over_time.csv")
# 
# #Untidy data for visualization
# df_plot <- df_corona_plot %>%   
#   gather('Count_Corona', 'Count_World', key = "Categories", value = "Total_Comments")
# 
# #Plot the Corona Concern
# ggplot(df_plot, aes(fill=Categories, y=Total_Comments, x=Date)) + 
#   geom_bar(position="dodge", stat="identity") +geom_line(aes(x=Date, y=Total_Comments, group=Categories))+
#   geom_point()
```

### 2. Reddit Sentiment Machine Learning

#### A. Preprocess Data for Machine Learning

Pulling the sentiment results together for the individual comments for machine learning.

```{r create_df0}
sent_reviews <- sent_reviews %>%
  mutate(id = as.integer(id))

df0 <- sent_reviews %>%
  select("id", "comment_score", "post_score") %>%
  unique()

nrcs <- sent_reviews %>%
  select("nrc") %>%
  unique() %>%
  na.omit() %>%
  as.list()

for (nrcx in nrcs$nrc) {
  df1 <- sent_reviews %>%
    select("id", "nrc") %>%
    filter(nrc == nrcx) %>%
    group_by(id) %>%
    summarise(num = n()) %>%
    arrange(id)
  
  colnames(df1)[2] <- paste0("nrc_", nrcx)
  
  df0 <- full_join(df0, df1, by = "id")
}
```

Preparing the data for machine learning.

```{r preprocess_df0}
x_train_tbl <- df0 %>% select(-c(comment_score, id))
y_train_tbl <- df0 %>% select(comment_score)

x_train_tbl_skim = partition(skim(x_train_tbl))

rec_obj <- recipe(~ ., data = x_train_tbl) %>%
  step_meanimpute(all_numeric()) %>%
  prep(training = x_train_tbl)

x_train_processed_tbl <- bake(rec_obj, x_train_tbl)
```

#### B. Create Machine Learning Model

Initiate an H2O Intance

```{r h2o_init, warning=FALSE, message=FALSE, results="hide"}
h2o.init(nthreads = -1)
h2o.removeAll()

data_h2o <- as.h2o(
  bind_cols(y_train_tbl, x_train_processed_tbl),
  destination_frame= "train.hex"
)
```

Train, Test, Validate Split in 70%, 15%, and 15%.

```{r data_split}
splits <- h2o.splitFrame(data = data_h2o,
                         ratios = c(0.7, 0.15),
                         seed = 1234)
train_h2o <- splits[[1]]
valid_h2o <- splits[[2]]
test_h2o  <- splits[[3]]
```

Build Machine Learning models

```{r h2o_run, warning=FALSE, message=FALSE, results="hide"}
y <- "comment_score"
x <- setdiff(names(train_h2o), y)

hyper_prms <- list(
  ntrees = c(45, 50, 55),
  learn_rate = c(0.1, 0.2),
  max_depth = c(20, 25)
)

grid_gbm <- h2o.grid(
  grid_id="grid_gbm",
  algorithm = "gbm",
  training_frame = train_h2o,
  validation_frame = valid_h2o,
  x = x,
  y = y,
  sample_rate = 0.7,
  col_sample_rate = 0.7,
  stopping_rounds = 2,
  stopping_metric = "RMSE",
  stopping_tolerance = 0.0001,
  score_each_iteration = T,
  model_id = "gbm_covType3",
  balance_classes = T,
  seed = 5000,
  hyper_params = hyper_prms
)
```

Choose the best model out of grid

```{r h2o_results}
grid <- h2o.getGrid("grid_gbm", sort_by = "rmse", decreasing = FALSE)
dl_grid_best_model <- h2o.getModel(grid@summary_table$model_ids[1])
```
