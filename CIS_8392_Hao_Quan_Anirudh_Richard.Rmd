---
title: "REDDIT SENTIMENT PROJECT"
author: "Richard More | Quan Le | Hao Nguyen | Anirudh Chaudhary"
date: "Date: 02/26/2020"
output: 
  html_document:
    theme: readable
    highlight: zenburn
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: false
header-includes:
  - \hypersetup{colorlinks=true, linkcolor=blue}
  - \usepackage{sectsty}
---

\allsectionsfont{\centering}
\subsectionfont{\raggedright}
\subsubsectionfont{\raggedleft}

<style type="text/css">

h1.title {
  font-size: 35px;
  color: DarkRed;
  text-align: center;
}
h4.author { 
    font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date { 
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}


</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### I. INTRODUCTION

#### 1. Overview 

<p>
We choose <https://www.reddit.com/> as our primary platform for the sentiment analysis. We choose Corona virus as our subreddit due to the current outbreak. We would like to explore what is everyone main reaction to this topic.
</p>

#### 2. Data Source 

<p>
We will use RedditExtractoR, tidyverse, and other relevant libraries. RedditExtractoR or Reddit Data Extraction Toolkit is created by Ivan Rivera where we can fetch the thread and content through the reddit URL. For more information, please visit <https://www.reddit.com/dev/api>.
</p>

#### 3. Summary 

<p>
Comments, Comment's scores, and post comments were the three main data that we retrieve for Corona Virus. After cleaning the stopwords and joinging the tokens with  Bing, NRC, and Afinn, we analyzed the trend of Corona Virus through the distribution of Sentiments and comments about Corona and World news over time. 
</p>

#### 4. Modeling 

<p>
Based on these analysis, we try to predict the comment scores with Gradient Boosting Machines and Linear Regression with the H2O instance. 
</p>

#### 5. Libraries

<p> The following libraries will be used in our projects </p>

```{r libs, warning=FALSE, message=FALSE}
library(RedditExtractoR)
library(tidyverse)
library(wordcloud)
library(tidytext)
library(recipes)
library(skimr)
library(h2o)
library(RCurl)
```

### II. REDDIT SENTIMENT ANALYSIS

#### 1. Retrieve Reddit Information

```{r gather_data, warning=FALSE, message=FALSE, results="hide"}
r <- reddit_urls(subreddit = "Coronavirus", page_threshold = 20)
rc <- reddit_content(r$URL)
rc_rel <- rc[c("comment", "comment_score", "post_score")]
rc_rel <- mutate(rc_rel, id = rownames(rc_rel))
```

#### 2. Clean and Transform Sentiment

```{r claning}
tokens <- rc_rel %>%
  unnest_tokens(output = word, input = comment)

sw = get_stopwords()
cleaned_tokens <- tokens %>%
  filter(!word %in% sw$word)

nums <- cleaned_tokens %>%
  filter(str_detect(word, "^[0-9]|http[s]?|.com$")) %>%
  select(word) %>% unique()

cleaned_tokens <- cleaned_tokens %>%
  filter(!word %in% nums$word)

rare <- cleaned_tokens %>%
  count(word) %>%
  filter(n < 10) %>%
  select(word) %>%
  unique()

cleaned_tokens <- cleaned_tokens %>%
  filter(!word %in% rare$word)
```

#### 3. WordCloud Visualization of World News

Wordcloud all cleaned tokens

```{r wordcloud}
pal <- brewer.pal(8,"Dark2")
cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = pal))
```

Join tokens with Bing, NRC, and Afinn

```{r sentiments, warning=FALSE, message=FALSE}
sent_reviews = cleaned_tokens %>%
  left_join(get_sentiments("nrc")) %>%
  rename(nrc = sentiment) %>%
  left_join(get_sentiments("bing")) %>%
  rename(bing = sentiment) %>%
  left_join(get_sentiments("afinn")) %>%
  rename(afinn = value)
```

#### 4. WordCloud Visualization of Corona Virus

Sentiment for the word "corona" is not present in the above mentioned 3 sentiment lexicons, so we have to add our own.

```{r corona_sentiment}
sent_reviews <- sent_reviews %>%
  mutate(bing = ifelse(word == "corona", "negative", bing)) %>%
  mutate(nrc = ifelse(word == "corona", "negative", nrc))
```

Wordcloud with Bing Lexicon

```{r sentiment_bing}
bing_word_counts <- sent_reviews %>%
  filter(!is.na(bing)) %>%
  count(word, bing, sort = TRUE)

bing_word_counts %>%
  filter(n > 250) %>%
  mutate(n = ifelse(bing == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = bing)) +  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

Wordcloud with NRC Lexicon

```{r sentiment_nrc_wordcloud}
nrc_word_counts <- sent_reviews %>%
  filter(!is.na(nrc)) %>%
  count(word, nrc, sort = TRUE)

nrc_word_counts %>%
  filter(n > 250) %>%
  mutate(n = ifelse(nrc %in% c("negative", "fear", "anger", "sadness", "disgust"), -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = nrc)) +  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

Distribution of sentiments

```{r sentiment_nrc_graph}
sent_reviews %>%
  filter(!is.na(nrc)) %>%
  group_by(nrc) %>%
  summarise(count = n()) %>%
  ggplot(., aes(x = reorder(nrc, -count), y = count)) +
    geom_bar(stat = "identity") +
    xlab("NRC") +
    labs(title = "Distribution of sentiments")
```

#### 5. Chart for Concern of Corona Virus Overtime on Reddit

In this section we are....
The following 2 code chunks are not run when creating the final HTML.
Set up the code to aggregate number of comment about World News and Corona everyday

```{r csv_creation, eval=FALSE}
df_corona <- rc %>%
  filter(str_detect(title,"corona|Corona|CORONA|2019-nCOV") |
         str_detect(title,"COVID-19|Covid-19|covid-19")) %>%
  select(comm_date) %>%
  group_by(comm_date) %>% 
  count(comm_date)

colnames(df_corona) <- c("Date", "Count_Corona")

df_world <- rc %>% 
  select(comm_date) %>%
  group_by(comm_date) %>%
  count(comm_date)

colnames(df_world) <- c("Date", "Count_World")

df_export <- inner_join(df_world, df_corona, by = "Date")

if (file.exists("corona_over_time.csv") == TRUE){
  write.table(df_export,"corona_over_time.csv",
              append = TRUE, col.names = FALSE, sep = ",", row.names = FALSE)
} else {
  write.table(df_export,"corona_over_time.csv",
              append = FALSE, col.names = TRUE, sep = ",", row.names = FALSE)
}
```

Code to create a schedule for Corona aggregation
Set schedule everyday to feed data

```{r scheduler, eval=FALSE}
library(taskscheduleR)
myscript <- system.file("reddit_corona.R")

taskscheduler_create(taskname = "CORONA_UPDATE",
                     rscript = "C:/Users/JNK4QBK/Desktop/reddit_corona.R",
                     schedule = "DAILY",
                     starttime = "23:50",
                     startdate = format(Sys.Date() + 1, "%d/%m/%Y"))
```

Import csv of Corona aggregation and visualize by barchart + line
```{r  fig.width=10, fig.height=8}
urlfile="https://raw.githubusercontent.com/hnguyen154/RedditBigDataProject/master/corona_over_time.csv"

x <- getURL(urlfile)
df_corona_plot <- read.csv(text = x)

df_plot <- df_corona_plot %>%
  gather("Count_Corona", "Count_World", key = "Categories", value = "Total_Comments")

ggplot(df_plot, aes(x = Date, y = Total_Comments, fill = Categories)) +
  geom_bar(position = "dodge", stat = "identity") +
  geom_line(aes(x=Date, y=Total_Comments, group=Categories)) +
  geom_point()
```

### III. REDDIT SENTIMENT MACHINE LEARNING

#### 1. Preprocess Data for Machine Learning

Pulling the sentiment results together for the individual comments for machine learning.

```{r create_df0}
sent_reviews <- sent_reviews %>%
  mutate(id = as.integer(id))

df0 <- sent_reviews %>%
  select("id", "comment_score", "post_score") %>%
  unique()

nrcs <- sent_reviews %>%
  select("nrc") %>%
  unique() %>%
  na.omit() %>%
  as.list()

for (nrcx in nrcs$nrc) {
  df1 <- sent_reviews %>%
    select("id", "nrc") %>%
    filter(nrc == nrcx) %>%
    group_by(id) %>%
    summarise(num = n()) %>%
    arrange(id)
  
  colnames(df1)[2] <- paste0("nrc_", nrcx)
  
  df0 <- full_join(df0, df1, by = "id")
}
```

Preparing the data for machine learning.

```{r preprocess_df0}
x_train_tbl <- df0 %>% select(-c(comment_score, id))
y_train_tbl <- df0 %>% select(comment_score)

x_train_tbl_skim = partition(skim(x_train_tbl))

rec_obj <- recipe(~ ., data = x_train_tbl) %>%
  step_meanimpute(all_numeric()) %>%
  prep(training = x_train_tbl)

x_train_processed_tbl <- bake(rec_obj, x_train_tbl)
```

#### 2. Create Machine Learning Model

Initiate an H2O Intance

```{r h2o_init, warning=FALSE, message=FALSE, results="hide"}
h2o.init(nthreads = -1)
h2o.removeAll()

data_h2o <- as.h2o(
  bind_cols(y_train_tbl, x_train_processed_tbl),
  destination_frame= "train.hex"
)
```

Train, Test, Validate Split in 70%, 15%, and 15%.

```{r data_split}
splits <- h2o.splitFrame(data = data_h2o,
                         ratios = c(0.7, 0.15),
                         seed = 1234)
train_h2o <- splits[[1]]
valid_h2o <- splits[[2]]
test_h2o  <- splits[[3]]
```

#### 3. Gradient Boosting Model

```{r h2o_run, warning=FALSE, message=FALSE, results="hide"}
y <- "comment_score"
x <- setdiff(names(train_h2o), y)

hyper_prms <- list(
  ntrees = c(45, 50, 55),
  learn_rate = c(0.1, 0.2),
  max_depth = c(20, 25)
)

grid_gbm <- h2o.grid(
  grid_id="grid_gbm",
  algorithm = "gbm",
  training_frame = train_h2o,
  validation_frame = valid_h2o,
  x = x,
  y = y,
  sample_rate = 0.7,
  col_sample_rate = 0.7,
  stopping_rounds = 2,
  stopping_metric = "RMSE",
  stopping_tolerance = 0.0001,
  score_each_iteration = T,
  model_id = "gbm_covType3",
  balance_classes = T,
  seed = 5000,
  hyper_params = hyper_prms
)
```

Choose the best model out of grid

```{r h2o_results}
grid <- h2o.getGrid("grid_gbm", sort_by = "rmse", decreasing = FALSE)
dl_grid_best_model <- h2o.getModel(grid@summary_table$model_ids[1])
dl_grid_best_model
```

#### 4. Linear Regression Model

```{r h2o_linearRegression, warning=FALSE, message=FALSE}
hyper_params <- list( alpha = c(0, .25, .5, .75, .1) )

# build grid search with previously selected hyperparameters
grid <- h2o.grid(x = x, y = y, training_frame = train_h2o, validation_frame = valid_h2o,
                 algorithm = "glm", grid_id = "grid_lr", hyper_params = hyper_params,
                 search_criteria = list(strategy = "Cartesian"))
  
```  

Choose the best model out of grid

```{r h2o_lr_results}
grid <- h2o.getGrid("grid_lr", sort_by = "rmse", decreasing = FALSE)
dl_grid_best_model <- h2o.getModel(grid@summary_table$model_ids[1])
dl_grid_best_model 
```


#### 5. Result and Discussion

After we fit the model Linear Regression and Gradient Boosting model, we attempt to find the best model based on the RMSE and R-Square in the report on validation data. However, the data shows a low R-Square and high RMSE. 