---
title: "REDDIT SENTIMENT PROJECT"
author: "Richard More | Quan Le | Hao Nguyen | Anirudh Chaudhary"
date: "Date: 02/26/2020"
output:
  html_document:
    theme: readable
    highlight: zenburn
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
header-includes:
  - \hypersetup{colorlinks=true, linkcolor=blue}
  - \usepackage{sectsty}
---

\allsectionsfont{\centering}
\subsectionfont{\raggedright}
\subsubsectionfont{\raggedleft}

<style type="text/css">

h1.title {
  font-size: 35px;
  color: DarkRed;
  text-align: center;
}
h4.author {
    font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h4.date {
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### I. INTRODUCTION

#### 1. Overview

<p>
We choose <https://www.reddit.com/> as our primary platform for the sentiment analysis. We chose Corona virus as our subreddit due to the current outbreak. We would like to explore what is the Reddit community's main reaction to this topic.
</p>

#### 2. Data Source

<p>
We will use RedditExtractoR, tidyverse, and other relevant libraries. RedditExtractoR (or Reddit Data Extraction Toolkit is created by Ivan Rivera) is the package with which we can fetch the thread and content through the Reddit URL. For more information, please visit <https://www.reddit.com/dev/api>.
</p>

#### 3. Summary

<p>
Comments, Comment's scores, and post scores are the attributes we will work with. After cleaning the stopwords and joinging the tokens with  Bing, NRC, and Afinn, we analyzed the trend of Corona Virus through the distribution of Sentiments and comments about Corona and World news over time.
</p>

#### 4. Modeling

<p>
Based on these analysis, we try to predict the comment scores with Gradient Boosting Machines and Linear Regression with the H2O instance.
</p>

#### 5. Libraries

<p>
The following libraries will be used in our projects
</p>

```{r libs, warning=FALSE, message=FALSE}
library(RedditExtractoR)
library(kableExtra)
library(tidyverse)
library(wordcloud)
library(ggthemes)
library(tidytext)
library(recipes)
library(skimr)
library(h2o)
library(RCurl)
```

### II. REDDIT SENTIMENT ANALYSIS

#### A. Retrieve Reddit Information

```{r gather_data, warning=FALSE, message=FALSE, results="hide"}
r <- reddit_urls(subreddit = "Coronavirus", page_threshold = 20)
rc <- reddit_content(r$URL)
```

```{r extract_data, warning=FALSE, message=FALSE}
rc_rel <- rc[c("comment", "comment_score", "post_score")]
rc_rel <- mutate(rc_rel, id = rownames(rc_rel))

rc_rel[, c(4, 3, 2, 1)] %>%
  head(n = 5) %>%
  kable() %>%
  add_header_above(header = c("Initial look at the data" = length(rc_rel)),
                   bold = TRUE,
                   font_size = "larger",
                   align = "left") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

#### B. Clean and Transform Sentiment

```{r claning}
tokens <- rc_rel %>%
  unnest_tokens(output = word, input = comment)

sw = get_stopwords()
cleaned_tokens <- tokens %>%
  filter(!word %in% sw$word)

nums <- cleaned_tokens %>%
  filter(str_detect(word, "^[0-9]|http[s]?|.com$")) %>%
  select(word) %>% unique()

cleaned_tokens <- cleaned_tokens %>%
  filter(!word %in% nums$word)

rare <- cleaned_tokens %>%
  count(word) %>%
  filter(n < 10) %>%
  select(word) %>%
  unique()

cleaned_tokens <- cleaned_tokens %>%
  filter(!word %in% rare$word)
```

#### C. WordCloud Visualization

Wordcloud all cleaned tokens

```{r wordcloud}
pal <- brewer.pal(8,"Dark2")
cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors = pal))
```

As we can see the main words are: people, virus, coronavirus, china, cases, etc. We can also find other words related to a virus: health, pandemic, infected, symptoms, spread.

Interestingly we can also determine which countries the virus has already spread: Italy, Korea, Iran, and of course China. The word "us" is not clear without context.

Join tokens with Bing, NRC, and Afinn

```{r sentiments, warning=FALSE, message=FALSE}
sent_reviews = cleaned_tokens %>%
  left_join(get_sentiments("nrc")) %>%
  rename(nrc = sentiment) %>%
  left_join(get_sentiments("bing")) %>%
  rename(bing = sentiment) %>%
  left_join(get_sentiments("afinn")) %>%
  rename(afinn = value)
```

#### D. Sentiment Visualization

Sentiment for the word "corona" is not present in the above mentioned 3 sentiment lexicons, so we have to add our own.

```{r corona_sentiment}
sent_reviews <- sent_reviews %>%
  mutate(bing = ifelse(word == "corona", "negative", bing)) %>%
  mutate(nrc = ifelse(word == "corona", "negative", nrc))
```

##### Bing Lexicon visualization

```{r sentiment_bing}
bing_word_counts <- sent_reviews %>%
  filter(!is.na(bing)) %>%
  count(word, bing, sort = TRUE)

bing_word_counts %>%
  filter(n > 250) %>%
  mutate(n = ifelse(bing == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = bing)) +
    geom_col() +
    coord_flip() +
    labs(y = "Contribution to sentiment") +
    theme_economist()
```

##### NRC Lexicon visualization

```{r sentiment_nrc_wordcloud}
nrc_word_counts <- sent_reviews %>%
  filter(!is.na(nrc)) %>%
  count(word, nrc, sort = TRUE)

nrc_word_counts %>%
  filter(n > 250) %>%
  mutate(n = ifelse(nrc %in%
                      c("negative", "fear", "anger", "sadness", "disgust"),
                    -n,
                    n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = nrc)) +
    geom_col() +
    coord_flip() +
    labs(y = "Contribution to sentiment") +
    theme_economist()
```

##### Distribution of sentiments

```{r sentiment_nrc_graph}
sent_reviews %>%
  filter(!is.na(nrc)) %>%
  group_by(nrc) %>%
  summarise(count = n()) %>%
  ggplot(., aes(x = reorder(nrc, -count), y = count)) +
    geom_bar(stat = "identity") +
    xlab("NRC") +
    ylab("Count") +
    labs(title = "Distribution of sentiments") +
    theme_economist()
```

#### E. Chart for Concern of Corona Virus Overtime on Reddit

In this section we are analyzing the trend about the Corona virus.

The following 2 code chunks are not run when creating the final HTML, but are used to create the CSV for aggregation.

Set up the code to aggregate number of comment about World News and Corona everyday

```{r csv_creation, eval=FALSE}
df_corona <- rc %>%
  filter(str_detect(title,"corona|Corona|CORONA|2019-nCOV") |
         str_detect(title,"COVID-19|Covid-19|covid-19")) %>%
  select(comm_date) %>%
  group_by(comm_date) %>%
  count(comm_date)

colnames(df_corona) <- c("Date", "Count_Corona")

df_world <- rc %>%
  select(comm_date) %>%
  group_by(comm_date) %>%
  count(comm_date)

colnames(df_world) <- c("Date", "Count_World")

df_export <- inner_join(df_world, df_corona, by = "Date")

if (file.exists("corona_over_time.csv") == TRUE){
  write.table(df_export,"corona_over_time.csv",
              append = TRUE, col.names = FALSE, sep = ",", row.names = FALSE)
} else {
  write.table(df_export,"corona_over_time.csv",
              append = FALSE, col.names = TRUE, sep = ",", row.names = FALSE)
}
```

Code to create a daily schedule for Corona aggregation

```{r scheduler, eval=FALSE}
library(taskscheduleR)
myscript <- system.file("reddit_corona.R")

taskscheduler_create(taskname = "CORONA_UPDATE",
                     rscript = "C:/Users/JNK4QBK/Desktop/reddit_corona.R",
                     schedule = "DAILY",
                     starttime = "23:50",
                     startdate = format(Sys.Date() + 1, "%d/%m/%Y"))
```

Import csv of Corona aggregation and visualize the results from the past several days

```{r csv_part}
urlfile="https://raw.githubusercontent.com/hnguyen154/RedditBigDataProject/master/corona_over_time.csv"

x <- getURL(urlfile)
df_corona_plot <- read.csv(text = x)

df_plot <- df_corona_plot %>%
  gather("Count_Corona", "Count_World", key = "Categories", value = "Total_Comments")

ggplot(df_plot, aes(x = Date, y = Total_Comments, fill = Categories)) +
  geom_bar(position = "dodge", stat = "identity") +
  geom_line(aes(x=Date, y=Total_Comments, group=Categories)) +
  geom_point() +
  ylab("Number of comments") +
  labs(title = "Corona virus comment trend") +
  theme_economist() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0, vjust = 0.5))
```

### III. REDDIT SENTIMENT MACHINE LEARNING

#### A. Preprocess Data for Machine Learning

Pulling the sentiment results together for the individual comments for machine learning.

```{r create_df0}
sent_reviews <- sent_reviews %>%
  mutate(id = as.integer(id))

df0 <- sent_reviews %>%
  select("id", "post_score", "comment_score") %>%
  unique()

nrcs <- sent_reviews %>%
  select("nrc") %>%
  unique() %>%
  na.omit() %>%
  as.list()

for (nrcx in nrcs$nrc) {
  df1 <- sent_reviews %>%
    select("id", "nrc") %>%
    filter(nrc == nrcx) %>%
    group_by(id) %>%
    summarise(num = n()) %>%
    arrange(id)

  colnames(df1)[2] <- paste0("nrc_", nrcx)

  df0 <- full_join(df0, df1, by = "id")
}

df0 %>%
  head(n = 5) %>%
  kable() %>%
  add_header_above(header = c("Data for Machine Learning" = length(df0)),
                   bold = TRUE,
                   font_size = "larger",
                   align = "left") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

Preparing the data for machine learning.

```{r preprocess_df0}
x_train_tbl <- df0 %>% select(-c(comment_score, id))
y_train_tbl <- df0 %>% select(comment_score)

x_train_tbl_skim = partition(skim(x_train_tbl))

rec_obj <- recipe(~ ., data = x_train_tbl) %>%
  step_meanimpute(all_numeric()) %>%
  prep(training = x_train_tbl)

x_train_processed_tbl <- bake(rec_obj, x_train_tbl)
```

#### B. Create Machine Learning Model

Initiate an H2O Intance

```{r h2o_init, warning=FALSE, message=FALSE, results="hide"}
h2o.init(nthreads = -1)
h2o.removeAll()

data_h2o <- as.h2o(
  bind_cols(y_train_tbl, x_train_processed_tbl),
  destination_frame= "train.hex"
)
```

Train, Test, Validate Split in 70%, 15%, and 15%.

```{r data_split}
splits <- h2o.splitFrame(data = data_h2o,
                         ratios = c(0.7, 0.15),
                         seed = 1234)
train_h2o <- splits[[1]]
valid_h2o <- splits[[2]]
test_h2o  <- splits[[3]]
```

#### C. Gradient Boosting Model

```{r h2o_gbm, warning=FALSE, message=FALSE, results="hide"}
y <- "comment_score"
x <- setdiff(names(train_h2o), y)

hyper_prms <- list(
  ntrees = c(45, 50, 55),
  learn_rate = c(0.1, 0.2),
  max_depth = c(20, 25)
)

grid_gbm <- h2o.grid(
  grid_id="grid_gbm",
  algorithm = "gbm",
  training_frame = train_h2o,
  validation_frame = valid_h2o,
  x = x,
  y = y,
  sample_rate = 0.7,
  col_sample_rate = 0.7,
  stopping_rounds = 2,
  stopping_metric = "RMSE",
  stopping_tolerance = 0.0001,
  score_each_iteration = T,
  model_id = "gbm_covType3",
  balance_classes = T,
  seed = 5000,
  hyper_params = hyper_prms
)
```

Get the best model and the metrics for that model from the grid

```{r h2o_results}
gbm_grid <- h2o.getGrid("grid_gbm", sort_by = "rmse", decreasing = FALSE)
gbm_best_model <- h2o.getModel(gbm_grid@summary_table$model_ids[1])

gbm_metrics.df <- tibble(metric = character(), train = double(), valid = double())
gbm_metrics.df <- add_row(gbm_metrics.df,
                      metric = "R^2",
                      train = h2o.r2(gbm_best_model, train = TRUE),
                      valid = h2o.r2(gbm_best_model, valid = TRUE)
                      )
gbm_metrics.df <- add_row(gbm_metrics.df,
                      metric = "MSE",
                      train = h2o.mse(gbm_best_model, train = TRUE),
                      valid = h2o.mse(gbm_best_model, valid = TRUE)
                      )
gbm_metrics.df <- add_row(gbm_metrics.df,
                      metric = "RMSE",
                      train = h2o.rmse(gbm_best_model, train = TRUE),
                      valid = h2o.rmse(gbm_best_model, valid = TRUE)
                      )
gbm_metrics.df <- add_row(gbm_metrics.df,
                      metric = "Mean Residual Deviance",
                      train = h2o.mean_residual_deviance(gbm_best_model, train = TRUE),
                      valid = h2o.mean_residual_deviance(gbm_best_model, valid = TRUE)
                      )

gbm_metrics.df %>%
  kable() %>%
  add_header_above(header = c("Metrics" = length(gbm_metrics.df)),
                   bold = TRUE,
                   font_size = "larger") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

#### D. Linear Regression Model

```{r h2o_lr, warning=FALSE, message=FALSE, results="hide"}
hyper_params <- list(
  alpha = c(0, .25, .5, .75, .1)
)

grid <- h2o.grid(
  grid_id = "grid_lr",
  algorithm = "glm",
  training_frame = train_h2o,
  validation_frame = valid_h2o,
  x = x,
  y = y,
  hyper_params = hyper_params,
  search_criteria = list(strategy = "Cartesian")
)
```

Get the best model and the metrics for that model from the grid

```{r h2o_lr_results}
lr_grid <- h2o.getGrid("grid_lr", sort_by = "rmse", decreasing = FALSE)
lr_best_model <- h2o.getModel(lr_grid@summary_table$model_ids[1])

lr_metrics.df <- tibble(metric = character(), train = double(), valid = double())
lr_metrics.df <- add_row(lr_metrics.df,
                      metric = "R^2",
                      train = h2o.r2(lr_best_model, train = TRUE),
                      valid = h2o.r2(lr_best_model, valid = TRUE)
                      )
lr_metrics.df <- add_row(lr_metrics.df,
                      metric = "MSE",
                      train = h2o.mse(lr_best_model, train = TRUE),
                      valid = h2o.mse(lr_best_model, valid = TRUE)
                      )
lr_metrics.df <- add_row(lr_metrics.df,
                      metric = "RMSE",
                      train = h2o.rmse(lr_best_model, train = TRUE),
                      valid = h2o.rmse(lr_best_model, valid = TRUE)
                      )
lr_metrics.df <- add_row(lr_metrics.df,
                      metric = "Mean Residual Deviance",
                      train = h2o.mean_residual_deviance(lr_best_model, train = TRUE),
                      valid = h2o.mean_residual_deviance(lr_best_model, valid = TRUE)
                      )

lr_metrics.df %>%
  kable() %>%
  add_header_above(header = c("Metrics" = length(lr_metrics.df)),
                   bold = TRUE,
                   font_size = "larger") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


#### E. Result and Discussion

After we fit the model Linear Regression and Gradient Boosting model, we attempt to find the best model based on the RMSE in the report on validation data. The results show low R-Square and high RMSE for both GBM and Linear Model.

